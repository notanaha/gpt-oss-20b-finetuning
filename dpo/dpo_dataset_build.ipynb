{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757399859926
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "SRC_JSONL = \"apto_reasoning_harmony.jsonl\"  # Existing SFT data (contains a messages array)\n",
        "OUT_PARQUET = \"dpo_pairs.parquet\"\n",
        "OUT_JSONL   = \"dpo_pairs.jsonl\"\n",
        "\n",
        "# 1) Load SFT dataset\n",
        "ds = load_dataset(\"json\", data_files=SRC_JSONL, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757399881133
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\", trust_remote_code=True)\n",
        "\n",
        "# Gather input texts first\n",
        "inputs_txt = []\n",
        "chosen_txt = []\n",
        "prompt_txt = []\n",
        "\n",
        "def first_or_none(msgs, role):\n",
        "    for m in msgs:\n",
        "        if m.get(\"role\") == role and isinstance(m.get(\"content\"), str):\n",
        "            return m[\"content\"]\n",
        "    return None\n",
        "\n",
        "for ex in ds:  # ds: SFT data (has a messages array)\n",
        "    user = first_or_none(ex[\"messages\"], \"user\")\n",
        "    ans  = first_or_none(ex[\"messages\"], \"assistant\")\n",
        "    if not user or not ans:\n",
        "        continue\n",
        "\n",
        "    # Optionally add system\n",
        "    msgs = ex[\"messages\"]\n",
        "    if not any(m.get(\"role\") == \"system\" for m in msgs):\n",
        "        msgs = [{\"role\": \"system\", \"content\": \"reasoning language: Japanese\"},\n",
        "                {\"role\": \"user\", \"content\": user}]\n",
        "    else:\n",
        "        # If you want generation with only the user message:\n",
        "        msgs = [{\"role\": \"user\", \"content\": user}]\n",
        "\n",
        "    # Render to a single string (ensures str)\n",
        "    prompt_str = tokenizer.apply_chat_template(\n",
        "        msgs, add_generation_prompt=True, tokenize=False\n",
        "    )\n",
        "\n",
        "    # Extra sanitization just in case\n",
        "    if isinstance(prompt_str, str) and isinstance(ans, str):\n",
        "        inputs_txt.append(prompt_str)\n",
        "        chosen_txt.append(ans)\n",
        "        prompt_txt.append(user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757399885978
        }
      },
      "outputs": [],
      "source": [
        "# 2) Prepare the base model (recommend using an un-fine-tuned base)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"openai/gpt-oss-20b\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757400207611
        }
      },
      "outputs": [],
      "source": [
        "# Generation settings (add slight randomness)\n",
        "GEN_KW = dict(\n",
        "    max_new_tokens=500,\n",
        "    do_sample=True,\n",
        "    #temperature=1.2,\n",
        "    #top_p=0.95,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# 4) Create rejected responses via batch generation\n",
        "BATCH = 4\n",
        "prompts, chosens, rejecteds = [], [], []\n",
        "\n",
        "# Generation loop\n",
        "for i in range(0, len(inputs_txt), BATCH):\n",
        "    batch_txt = inputs_txt[i:i+BATCH]\n",
        "    # Safety: ensure all elements are str\n",
        "    batch_txt = [t for t in batch_txt if isinstance(t, str)]\n",
        "    if not batch_txt:\n",
        "        continue\n",
        "\n",
        "    enc = tokenizer(\n",
        "        batch_txt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,  # Safety truncation\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(**enc, **GEN_KW)\n",
        "\n",
        "    # 3) Slice out only the generated continuation using input attention_mask to get input length\n",
        "    for j in range(out_ids.size(0)):\n",
        "        in_len = int(enc[\"attention_mask\"][j].sum().item())\n",
        "        gen_ids = out_ids[j, in_len:]\n",
        "        gen_txt = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "        # Bounds check to keep original indexing alignment\n",
        "        idx = i + j\n",
        "        if idx < len(chosen_txt) and idx < len(prompt_txt):\n",
        "            prompts.append(prompt_txt[idx])\n",
        "            chosens.append(chosen_txt[idx])\n",
        "            rejecteds.append(gen_txt if gen_txt else \"(empty)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757400231256
        }
      },
      "outputs": [],
      "source": [
        "# 5) Build and save DPO dataset\n",
        "dpo_ds = Dataset.from_dict({\"prompt\": prompts, \"chosen\": chosens, \"rejected\": rejecteds})\n",
        "dpo_ds.to_parquet(OUT_PARQUET)\n",
        "# Also save human-readable JSONL (ensure_ascii=False for UTF-8)\n",
        "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in dpo_ds:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"DPO pairs saved: {OUT_PARQUET}, {OUT_JSONL}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
