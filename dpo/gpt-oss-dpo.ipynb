{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757670487386
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#from azure.ai.ml import MLClient, Input, MpiDistribution, command\n",
        "from azure.ai.ml import MLClient, Input, Output, PyTorchDistribution, command\n",
        "from azure.ai.ml.entities import (\n",
        "    AmlCompute, Environment, BuildContext, Data,\n",
        "    ManagedOnlineEndpoint, ManagedOnlineDeployment, CodeConfiguration, OnlineRequestSettings\n",
        ")\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import datetime\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Azure ML workspace configuration\n",
        "SUBSCRIPTION_ID = os.getenv(\"SUBSCRIPTION_ID\")\n",
        "RESOURCE_GROUP = os.getenv(\"RESOURCE_GROUP\")\n",
        "WORKSPACE_NAME = os.getenv(\"WORKSPACE_NAME\")\n",
        "COMPUTE_CLUSTER = \"demo-gpucluster01\"\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "\n",
        "# authentication via managed identity or service principal (no hard-coded creds)\n",
        "ml_client = MLClient(DefaultAzureCredential(), SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)\n",
        "\n",
        "# ensure compute cluster exists or create it\n",
        "try:\n",
        "    ml_client.compute.get(COMPUTE_CLUSTER)\n",
        "except Exception:\n",
        "    print(\"demo-gpucluster01 was not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757649926407
        }
      },
      "outputs": [],
      "source": [
        "data_uri = \"./dpo_pairs.parquet\"\n",
        "\n",
        "data = Data(\n",
        "    path = data_uri,\n",
        "    type = AssetTypes.URI_FILE,\n",
        "    description = \"dpo dataset for gpt-oss-20b\",\n",
        "    name = \"dpo_pairs\",\n",
        "    version = '1'\n",
        ")\n",
        "ml_client.data.create_or_update(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756866757124
        }
      },
      "outputs": [],
      "source": [
        "%%writefile ./src/dpo.py\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Mxfp4Config,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_file\", type=str, required=True,\n",
        "                   help=\"Converted Harmony (or DPO) data (parquet or jsonl)\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./outputs\",\n",
        "                   help=\"Output directory\")\n",
        "    parser.add_argument(\"--push_to_hub\", action=\"store_true\",\n",
        "                   help=\"Specify to push to Hugging Face Hub\")\n",
        "    parser.add_argument(\"--hub_model_id\", type=str, default=None,\n",
        "                   help=\"Model id on the Hub (org/name). If omitted, uses output_dir name\")\n",
        "    parser.add_argument(\"--hf_token\", type=str, default=None,\n",
        "                   help=\"Hugging Face access token (or via env var HF_TOKEN)\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=2e-4)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
        "    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=2)\n",
        "    parser.add_argument(\"--grad_accum_steps\", type=int, default=8)\n",
        "    parser.add_argument(\"--max_seq_len\", type=int, default=2048)\n",
        "    parser.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n",
        "    parser.add_argument(\"--cosine_min_lr_rate\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=10)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def load_dpo_dataset(train_file):\n",
        "    train_file = os.path.abspath(train_file)\n",
        "    if train_file.endswith(\".parquet\"):\n",
        "        ds = load_dataset(\"parquet\", data_files=train_file)[\"train\"]\n",
        "    elif train_file.endswith(\".jsonl\") or train_file.endswith(\".json\"):\n",
        "        ds = load_dataset(\"json\", data_files=train_file, split=\"train\")\n",
        "    else:\n",
        "        raise ValueError(\"train_file must be .parquet or .jsonl\")\n",
        "\n",
        "    needed = {\"prompt\", \"chosen\", \"rejected\"}\n",
        "    if not needed.issubset(ds.column_names):\n",
        "        raise ValueError(f\"DPO requires columns {needed}. Found: {ds.column_names}\")\n",
        "    return ds\n",
        "\n",
        "\n",
        "def ensure_chat_form(ds, tokenizer):\n",
        "    def _map(ex):\n",
        "        p = ex[\"prompt\"]\n",
        "        # Skip if already templated\n",
        "        if any(m in p for m in (\"<|assistant|>\", \"<|user|>\", \"<|system|>\", \"<|im_start|>\", \"<|start_header_id|>\")):\n",
        "            return {\"prompt\": p}\n",
        "        msgs = [{\"role\": \"user\", \"content\": p}]\n",
        "        templated = tokenizer.apply_chat_template(\n",
        "            msgs, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        return {\"prompt\": templated}\n",
        "    return ds.map(_map)\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = get_args()\n",
        "\n",
        "    # Hub token (needed only when pushing)\n",
        "    hf_token = args.hf_token or os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "    # Flexible dtype/device so it runs even on a CPU smoke test\n",
        "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    dtype = torch.bfloat16 if use_bf16 else torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    # In AzureML distributed environments accelerate is used; omit device_map here\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"openai/gpt-oss-20b\",\n",
        "        attn_implementation=\"eager\",\n",
        "        dtype=dtype,\n",
        "        #quantization_config=quantization_config,\n",
        "        use_cache=False,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"openai/gpt-oss-20b\",\n",
        "        trust_remote_code=True,\n",
        "        use_fast=True,\n",
        "    )\n",
        "    # Minimal settings for chat / long text\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=8, \n",
        "        lora_alpha=16, \n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"gate_up_proj\",\n",
        "            \"down_proj\"\n",
        "        ],\n",
        "        bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # --- Dataset ---\n",
        "    dpo_ds = load_dpo_dataset(args.train_file)\n",
        "    dpo_ds = ensure_chat_form(dpo_ds, tokenizer)\n",
        "\n",
        "    # --- DPO config ---\n",
        "    # report_to may be \"none\" / \"tensorboard\" / \"wandb\" etc.\n",
        "    dpo_args = DPOConfig(\n",
        "        learning_rate=args.lr,\n",
        "        gradient_checkpointing=True,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False},   # added\n",
        "        #ddp_find_unused_parameters=False,                        # may cause OOM\n",
        "        #packing=False,                                           # optional\n",
        "        beta=0.1,                                                 # DPO\n",
        "        loss_type=\"ipo\",                                          # DPO variant\n",
        "        max_length=args.max_seq_len,                              # DPO\n",
        "        max_prompt_length=min(1024, args.max_seq_len // 2),       # DPO\n",
        "        num_train_epochs=args.epochs,\n",
        "        logging_steps=args.logging_steps,\n",
        "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=args.grad_accum_steps,\n",
        "        warmup_ratio=args.warmup_ratio,\n",
        "        lr_scheduler_type=\"cosine_with_min_lr\",\n",
        "        lr_scheduler_kwargs={\"min_lr_rate\": args.cosine_min_lr_rate},\n",
        "        output_dir=args.output_dir,\n",
        "        report_to=\"trackio\", # changed from none\n",
        "        bf16=use_bf16,  # enable only if GPU supports bf16\n",
        "        fp16=(torch.cuda.is_available() and not use_bf16),\n",
        "        seed=args.seed,\n",
        "        push_to_hub=args.push_to_hub,\n",
        "        hub_model_id=(args.hub_model_id or os.path.basename(args.output_dir)) if args.push_to_hub else None,\n",
        "        hub_token=hf_token if args.push_to_hub else None,\n",
        "        save_total_limit=2,\n",
        "        save_steps=500,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    trainer = DPOTrainer(\n",
        "        model=model,         # Existing LoRA-applied model (or base + LoRA applied here)\n",
        "        args=dpo_args,\n",
        "        processing_class=tokenizer,\n",
        "        train_dataset=dpo_ds,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # === Merge LoRA into base model and save ===\n",
        "    try:\n",
        "        print(\"Merging LoRA adapter into base model...\")\n",
        "        merged_model = model.merge_and_unload()\n",
        "        merged_dir = os.path.join(args.output_dir, \"merged\")\n",
        "        os.makedirs(merged_dir, exist_ok=True)\n",
        "        merged_model.save_pretrained(merged_dir)\n",
        "        tokenizer.save_pretrained(merged_dir)\n",
        "        print(f\"Merged model saved at: {merged_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not merge model automatically: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757677928643
        }
      },
      "outputs": [],
      "source": [
        "# job configuration\n",
        "NUM_NODES = 2\n",
        "NUM_GPU_PER_NODE = 1\n",
        "\n",
        "# define distributed training job\n",
        "dist = PyTorchDistribution(\n",
        "    process_count_per_instance=NUM_GPU_PER_NODE,\n",
        "    node_count=NUM_NODES\n",
        ")\n",
        "\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",  # Root (refers to src/dpo.py)\n",
        "    command=(\n",
        "        \"python dpo.py \"\n",
        "        \"--train_file ${{inputs.train_file}} \"\n",
        "        #\"--output_dir ${{outputs.model_dir}} \"   # default to \"./outputs\"\n",
        "        \"--epochs 8 \" \n",
        "        \"--per_device_train_batch_size 1 \"\n",
        "        \"--grad_accum_steps 16 \" \n",
        "        \"--max_seq_len 1024 \"\n",
        "    ),\n",
        "    inputs={\n",
        "        \"train_file\": Input(\n",
        "            type=AssetTypes.URI_FILE,\n",
        "            path=\"dpo_pairs@latest\"\n",
        "        )\n",
        "    },\n",
        "    outputs={\"model_dir\": {\"mode\": \"rw_mount\", \"path\": \"azureml://datastores/workspaceblobstore/paths/models/gpt-oss-20b-jp-reasoner\"}},\n",
        "    environment=\"env-gpt-oss-01:2\",  # Includes transformers>=4.44, trl>=0.9, peft>=0.10, accelerate>=0.33\n",
        "    compute=COMPUTE_CLUSTER,\n",
        "    display_name=\"dpo-gpt-oss-20b-01\",\n",
        "    experiment_name=\"dpo-gpt-oss-20b-01-exp\",\n",
        "    instance_count=NUM_NODES,\n",
        "    distribution=dist,\n",
        "    environment_variables={\n",
        "        \"NCCL_DEBUG\": \"WARN\",\n",
        "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
        "        \"HF_TOKEN\": HF_TOKEN,\n",
        "        \"HF_HOME\": \"./outputs/hfhome\",\n",
        "        \"TRACKIO_PROJECT\": \"ft-project\"\n",
        "    }\n",
        ")\n",
        "returned_job = ml_client.jobs.create_or_update(job)\n",
        "print(returned_job.studio_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Register Model</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757676088580
        }
      },
      "outputs": [],
      "source": [
        "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
        "    returned_job.name, \"artifacts/outputs/merged\"\n",
        ")\n",
        "\n",
        "print(\"path to register model: \", model_path_from_job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1757676179675
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "model = Model(\n",
        "    path=model_path_from_job,  # Reference to training job output\n",
        "    name=\"gpt-oss-20b-jp-reasoner-01-dpo\",\n",
        "    description=\"LoRA fine-tuned gpt-oss-20b model for Japanese reasoning\",\n",
        "    type=\"custom_model\",   # Hugging Face models are often registered as custom_model\n",
        "    version=\"1\",             # Explicit version (or omit for auto increment)\n",
        ")\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model)\n",
        "print(f\"Registered: {registered_model.name}:{registered_model.version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Deploy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./src/score.py\n",
        "# score.py\n",
        "import os, json, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "REASONING_LANGUAGE_DEFAULT = \"Japanese\"\n",
        "\n",
        "def init():\n",
        "    global model, tokenizer\n",
        "    model_root = os.environ.get(\"AZUREML_MODEL_DIR\", \".\")\n",
        "    model_dir  = os.path.join(model_root, os.getenv(\"MODEL_SUBDIR\", \"merged\"))  # Read merged/\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_dir, trust_remote_code=True, local_files_only=True\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        trust_remote_code=True,\n",
        "        dtype=(torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else None),\n",
        "        device_map=(\"auto\" if torch.cuda.is_available() else None),\n",
        "        local_files_only=True,\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "def _build_messages(data):\n",
        "    # Input can be either 1) {\"prompt\": \"...\"} or 2) {\"messages\": [...]} \n",
        "    system_prompt = data.get(\n",
        "        \"system_prompt\",\n",
        "        f\"reasoning language: {data.get('reasoning_language', REASONING_LANGUAGE_DEFAULT)}\"\n",
        "    )\n",
        "\n",
        "    if \"messages\" in data and isinstance(data[\"messages\"], list):\n",
        "        msgs = data[\"messages\"]\n",
        "        # Prepend system if the first message isn't system\n",
        "        if not msgs or msgs[0].get(\"role\") != \"system\":\n",
        "            msgs = [{\"role\": \"system\", \"content\": system_prompt}] + msgs\n",
        "        return msgs\n",
        "\n",
        "    # Default messages if only a \"prompt\" was provided\n",
        "    user_prompt = data.get(\"prompt\", \"\")\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "def run(raw_data):\n",
        "    try:\n",
        "        data = json.loads(raw_data) if isinstance(raw_data, str) else raw_data\n",
        "\n",
        "        # ---- Build messages (based on provided code) ----\n",
        "        messages = _build_messages(data)\n",
        "\n",
        "        # Generation parameters (moderate defaults)\n",
        "        max_new = int(data.get(\"max_new_tokens\", 128))\n",
        "        do_sample = bool(data.get(\"do_sample\", False))\n",
        "        gen_kwargs = {\n",
        "            \"max_new_tokens\": max_new,\n",
        "            \"do_sample\": do_sample,\n",
        "        }\n",
        "        # Only set temperature if sampling; otherwise warning appears\n",
        "        if do_sample and \"temperature\" in data:\n",
        "            gen_kwargs[\"temperature\"] = float(data[\"temperature\"])\n",
        "\n",
        "        # ---- chat template -> input_ids ----\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(model.device)\n",
        "\n",
        "        # ---- Generate ----\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(input_ids, **gen_kwargs)\n",
        "\n",
        "        # ---- Remove prompt echo (keep only generated continuation) ----\n",
        "        generated_ids = output_ids[0, input_ids.shape[-1]:]\n",
        "        text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Return JSON (UTF-8)\n",
        "        return json.dumps({\"output\": text}, ensure_ascii=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return error also as JSON\n",
        "        return json.dumps({\"error\": str(e)}, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756961174089
        }
      },
      "outputs": [],
      "source": [
        "endpoint_name = f\"gptoss-jp-{datetime.datetime.now():%m%d%H%M}\"\n",
        "\n",
        "# 1) Create endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=endpoint_name, \n",
        "    description=\"gpt-oss-20b\",\n",
        "    auth_mode=\"key\")\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756968716845
        }
      },
      "outputs": [],
      "source": [
        "# 2) Deploy (refer to the already registered merged model in Model Registry)\n",
        "registered = ml_client.models.get(\"gpt-oss-20b-jp-reasoner-01-dpo\", version=\"1\")\n",
        "\n",
        "deploy = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=registered.id,\n",
        "    environment=\"env-gpt-oss-01@latest\",\n",
        "    code_configuration=CodeConfiguration(code=\"./src\", scoring_script=\"score.py\"),\n",
        "    instance_type=\"Standard_NC40ads_H100_v5\", \n",
        "    instance_count=1,\n",
        "    request_settings=OnlineRequestSettings(\n",
        "        request_timeout_ms=180000,\n",
        "        max_concurrent_requests_per_instance=1,\n",
        "        max_queue_wait_ms=180000,\n",
        "    ),\n",
        ")\n",
        "ml_client.begin_create_or_update(deploy).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756964014316
        }
      },
      "outputs": [],
      "source": [
        "# 3) Traffic routing\n",
        "ep = ml_client.online_endpoints.get(endpoint_name)\n",
        "ep.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(ep).result()\n",
        "\n",
        "print(\"endpoint:\", endpoint_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Try the deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756968996353
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import OnlineRequestSettings\n",
        "from azure.ai.ml import MLClient\n",
        "import json, requests\n",
        "\n",
        "keys = ml_client.online_endpoints.get_keys(name=endpoint_name).primary_key\n",
        "scoring_url = ml_client.online_endpoints.get(endpoint_name).scoring_uri\n",
        "headers = {\"Authorization\": f\"Bearer {keys}\"}\n",
        "\n",
        "payload = {\n",
        "  \"messages\": [\n",
        "    {\"role\": \"system\", \"content\": \"reasoning language: Japanese\"},\n",
        "    {\"role\": \"user\", \"content\": \"オーストラリアの首都はどこですか？\"}\n",
        "  ],\n",
        "  \"max_new_tokens\": 500\n",
        "}\n",
        "\n",
        "res = requests.post(scoring_url, headers=headers, json=payload, timeout=300)\n",
        "print(res.status_code)\n",
        "data = res.json()                 # First-level decode\n",
        "if isinstance(data, str):         # If still a string, decode second-level\n",
        "    data = json.loads(data)\n",
        "\n",
        "print(data[\"output\"])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756969946808
        }
      },
      "outputs": [],
      "source": [
        "ml_client.online_endpoints.begin_delete(name=endpoint_name).wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>問題解決"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756964968400
        }
      },
      "outputs": [],
      "source": [
        "print(res.status_code)\n",
        "print(res.headers.get(\"content-type\"))\n",
        "print(res.text[:1000])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756964883410
        }
      },
      "outputs": [],
      "source": [
        "ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=endpoint_name, lines=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Original Code</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1747050409131
        }
      },
      "outputs": [],
      "source": [
        "# job configuration\n",
        "NUM_NODES = 1\n",
        "NUM_GPU_PER_NODE = 1\n",
        "\n",
        "# define distributed training job\n",
        "dist = PyTorchDistribution(\n",
        "    process_count_per_instance=NUM_GPU_PER_NODE,\n",
        "    node_count=NUM_NODES\n",
        ")\n",
        "\n",
        "job = command(\n",
        "    code=\"./azureml\",\n",
        "    command=(\n",
        "        \"python megatron_lm/tools/preprocess_data.py \\\n",
        "        --input ${{inputs.train_data}} \\\n",
        "        --output-prefix ${{outputs.indexed}}/wikidump \\\n",
        "        --tokenizer-type Llama2Tokenizer \\\n",
        "        --tokenizer-model ${{inputs.model_dir}} \\\n",
        "        --workers 1 && \"\n",
        "        \"cp ${{inputs.train_data}} ${{outputs.indexed}} && \"\n",
        "        \"mv ${{outputs.indexed}}/wikidump_text_document.bin ${{outputs.indexed}}/wikidump.jsonl.bin && \"\n",
        "        \"mv ${{outputs.indexed}}/wikidump_text_document.idx ${{outputs.indexed}}/wikidump.jsonl.idx\"\n",
        "    ),\n",
        "    inputs={\n",
        "        \"train_data\": Input(\n",
        "            type=AssetTypes.URI_FILE, \n",
        "            path=\"wiki_dump@latest\"\n",
        "        ),\n",
        "        \"model_dir\": Input(\n",
        "            type=AssetTypes.URI_FOLDER, \n",
        "            path=\"llama3-8b@latest\"\n",
        "        )\n",
        "    },\n",
        "    outputs={\n",
        "        \"indexed\": Output(\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            path=\"azureml://datastores/workspaceblobstore/paths/wiki-indexed-dataset1/\",\n",
        "            mode=\"rw_mount\"\n",
        "        )                      # Mountable from subsequent jobs\n",
        "    },\n",
        "    environment=\"llama3-8b-wiki_env@latest\",\n",
        "    compute=COMPUTE_CLUSTER,\n",
        "    instance_count=NUM_NODES,\n",
        "    distribution=dist,\n",
        "    environment_variables={\n",
        "        \"LOGLEVEL\": \"INFO\",\n",
        "        \"NCCL_DEBUG\": \"WARN\",\n",
        "        \"NCCL_DEBUG_SUBSYS\": \"WARN\",\n",
        "        \"PYTHONFAULTHANDLER\": \"1\",\n",
        "        \"CUDA_LAUNCH_BLOCKING\": \"0\"\n",
        "    },\n",
        "    display_name=\"llama3-8b-wiki-index\",\n",
        "    experiment_name=\"llama3-8b-wiki-index-exp\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1747050764449
        }
      },
      "outputs": [],
      "source": [
        "# submit the job\n",
        "returned_job = ml_client.jobs.create_or_update(job)\n",
        "print(f\"Job submitted: {returned_job.name}\")\n",
        "print(f\"Monitor at: {returned_job.studio_url}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
