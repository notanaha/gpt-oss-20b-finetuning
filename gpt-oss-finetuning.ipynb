{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756974916040
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.ai.ml import MLClient, Input, Output, PyTorchDistribution, command\n",
        "from azure.ai.ml.entities import (\n",
        "    AmlCompute, Environment, BuildContext, Data,\n",
        "    ManagedOnlineEndpoint, ManagedOnlineDeployment, CodeConfiguration, OnlineRequestSettings\n",
        ")\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import datetime\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Azure ML workspace configuration\n",
        "SUBSCRIPTION_ID = os.getenv(\"SUBSCRIPTION_ID\")\n",
        "RESOURCE_GROUP = os.getenv(\"RESOURCE_GROUP\")\n",
        "WORKSPACE_NAME = os.getenv(\"WORKSPACE_NAME\")\n",
        "COMPUTE_CLUSTER = \"demo-gpucluster01\"\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "\n",
        "# authentication via managed identity or service principal (no hard-coded creds)\n",
        "ml_client = MLClient(DefaultAzureCredential(), SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)\n",
        "\n",
        "# ensure compute cluster exists or create it\n",
        "try:\n",
        "    ml_client.compute.get(COMPUTE_CLUSTER)\n",
        "except Exception:\n",
        "    print(\"demo-gpucluster01 was not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Prepare Environment</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"environment\", exist_ok=True)\n",
        "\n",
        "%%writefile ./environment/Dockerfile\n",
        "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2204-cu118-py310-torch271:biweekly.202508.1\n",
        "\n",
        "# Install pip dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt --no-cache-dir\n",
        "# Upgrade known vulnerable packages again just to be safe\n",
        "RUN pip install --upgrade \\\n",
        "    requests==2.32.4 \\\n",
        "    urllib3==2.5.0 \\\n",
        "    pillow==11.3.0 || true\n",
        "\n",
        "# Repeat for other envs if applicable\n",
        "RUN /opt/conda/bin/pip install --upgrade \\\n",
        "    requests==2.32.4 \\\n",
        "    urllib3==2.5.0 \\\n",
        "    pillow==11.3.0 || true\n",
        "\n",
        "RUN /opt/conda/envs/ptca/bin/pip install --upgrade \\\n",
        "    requests==2.32.4 \\\n",
        "    urllib3==2.5.0 \\\n",
        "    pillow==11.3.0 || true\n",
        "\n",
        "# Inference requirements\n",
        "COPY --from=mcr.microsoft.com/azureml/o16n-base/python-assets:20230419.v1 /artifacts /var/\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y --no-install-recommends \\\n",
        "        libcurl4 \\\n",
        "        liblttng-ust1 \\\n",
        "        libunwind8 \\\n",
        "        libxml++2.6-2v5 \\\n",
        "        nginx-light \\\n",
        "        psmisc \\\n",
        "        rsyslog \\\n",
        "        runit \\\n",
        "        unzip && \\\n",
        "    apt-get clean && rm -rf /var/lib/apt/lists/* && \\\n",
        "    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\n",
        "    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\n",
        "    ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\n",
        "    rm -f /etc/nginx/sites-enabled/default\n",
        "\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y --only-upgrade \\\n",
        "        libpython3.10-stdlib \\\n",
        "        python3.10 \\\n",
        "        libpython3.10-minimal \\\n",
        "        python3.10-minimal \\\n",
        "        libpam0g \\\n",
        "        libpam-modules-bin \\\n",
        "        libpam-modules \\\n",
        "        libpam-runtime \\\n",
        "        sudo && \\\n",
        "    apt-get clean && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "ENV SVDIR=/var/runit\n",
        "ENV WORKER_TIMEOUT=400\n",
        "EXPOSE 5001 8883 8888\n",
        "\n",
        "# support Deepspeed launcher requirement of passwordless ssh login\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y openssh-server openssh-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./environment/requirements.txt\n",
        "azureml-core==1.60.0.post1\n",
        "azureml-dataset-runtime==1.60.0\n",
        "azureml-defaults==1.60.0\n",
        "azure-ml==0.0.1\n",
        "azure-ml-component==0.9.18.post2\n",
        "azureml-mlflow==1.60.0.post1\n",
        "azureml-contrib-services==1.60.0\n",
        "azureml-contrib-services==1.60.0\n",
        "azureml-inference-server-http\n",
        "inference-schema\n",
        "MarkupSafe==2.1.2\n",
        "regex\n",
        "pybind11\n",
        "urllib3==2.5.0\n",
        "requests==2.32.4\n",
        "pillow==11.3.0\n",
        "cryptography>=42.0.4\n",
        "aiohttp>=3.12.14\n",
        "py-spy==0.3.12\n",
        "debugpy~=1.6.3\n",
        "ipykernel~=6.0\n",
        "tensorboard\n",
        "psutil~=5.8.0\n",
        "matplotlib~=3.5.0\n",
        "tqdm~=4.66.3\n",
        "py-cpuinfo==5.0.0\n",
        "torch-tb-profiler~=0.4.0\n",
        "trl>=0.20.0\n",
        "peft>=0.17.0\n",
        "transformers>=4.55.0\n",
        "trackio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Create Environment</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756863634984
        }
      },
      "outputs": [],
      "source": [
        "env_name = \"env-gpt-oss-01\"\n",
        "docker_dir=\"./environment\"\n",
        "\n",
        "env_docker_image = Environment(\n",
        "    build=BuildContext(path=docker_dir),\n",
        "    name=env_name,\n",
        "    description=\"Environment created from a Docker context.\",\n",
        ")\n",
        "env_asset = ml_client.environments.create_or_update(env_docker_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756862984202
        }
      },
      "outputs": [],
      "source": [
        "#!pip install huggingface_hub\n",
        "#!pip install ipywidgets\n",
        "#!pip install transformers  # Uncomment if transformers not preinstalled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Login to Huggingface</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756863936063
        }
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Prepare Training Dataset</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756864124123
        }
      },
      "outputs": [],
      "source": [
        "# pip install datasets pandas pyarrow\n",
        "import re\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "APTO_DS = \"APTOinc/japanese-reasoning-dataset-sample\"  # Sample. Same applies to real data.\n",
        "\n",
        "# Extract <think> ... </think> and return (thinking, final)\n",
        "THINK_RE = re.compile(r\"<think>\\s*(.*?)\\s*</think>\\s*(.*)\", re.DOTALL)\n",
        "\n",
        "def split_think_final(answer_text: str):\n",
        "    if not answer_text:\n",
        "        return \"\", \"\"\n",
        "    m = THINK_RE.match(answer_text)\n",
        "    if m:\n",
        "        thinking = m.group(1).strip()\n",
        "        final = m.group(2).strip()\n",
        "    else:\n",
        "        # Fallback: if no <think> tag, leave thinking empty and treat entire text as final\n",
        "        thinking = \"\"\n",
        "        final = answer_text.strip()\n",
        "    return thinking, final\n",
        "\n",
        "# 1) Load APTO dataset\n",
        "ds = load_dataset(APTO_DS, split=\"train\")\n",
        "\n",
        "# 2) Assemble into Harmony format (messages column)\n",
        "def to_harmony(example):\n",
        "    thinking, final = split_think_final(example.get(\"answer\", \"\"))\n",
        "\n",
        "    # Following the Cookbook style, put 'reasoning language: Japanese' in the system message\n",
        "    # (Minimal structure aligned with Multilingual-Thinking and Cookbook examples)\n",
        "    messages = [\n",
        "        {\"role\": \"system\",\n",
        "         \"content\": \"reasoning language: Japanese\\n\\nYou are an AI chatbot with a lively and energetic personality.\"},\n",
        "        {\"role\": \"user\",\n",
        "         \"content\": example.get(\"question\", \"\").strip()},\n",
        "        # The gpt-oss chat template lets the assistant have both thinking and content\n",
        "        {\"role\": \"assistant\",\n",
        "         \"thinking\": thinking,\n",
        "         \"content\": final}\n",
        "    ]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "dataset = ds.map(to_harmony, remove_columns=ds.column_names)\n",
        "\n",
        "# 3) Save (either format is fine)\n",
        "# Parquet (Multilingual-Thinking in the Cookbook is distributed as parquet)\n",
        "dataset.to_parquet(\"apto_reasoning_harmony.parquet\")\n",
        "# JSONL (easier for humans to inspect)\n",
        "dataset.to_json(\"apto_reasoning_harmony.jsonl\", lines=True)\n",
        "\n",
        "# If you'd prefer readable Japanese...\n",
        "'''with open(\"apto_reasoning_harmony.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for ex in converted:\n",
        "        f.write(json.dumps(ex, ensure_ascii=False))\n",
        "        f.write(\"\\n\")'''\n",
        "\n",
        "print(dataset[0][\"messages\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756864128954
        }
      },
      "outputs": [],
      "source": [
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Register Dataset</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756873308057
        }
      },
      "outputs": [],
      "source": [
        "data_uri = \"./apto_reasoning_harmony.parquet\"\n",
        "\n",
        "data = Data(\n",
        "    path = data_uri,\n",
        "    type = AssetTypes.URI_FILE,\n",
        "    description = \"gpt-oss-20b\",\n",
        "    name = \"apto_reasoning_harmony\",\n",
        "    version = '1'\n",
        ")\n",
        "ml_client.data.create_or_update(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Peek the 1st record of the dataset using Chat Template in Tokenizer </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756864200105
        }
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
        "\n",
        "messages = dataset[0][\"messages\"]\n",
        "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Load model on this compute resource</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756865195404
        }
      },
      "outputs": [],
      "source": [
        "# LOAD MODEL ON THIS COMPUTE INSTANCE\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
        "\n",
        "quantization_config = Mxfp4Config(dequantize=True)\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config,\n",
        "    use_cache=False,\n",
        "    #device_map=\"auto\",\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs)\n",
        "model.to(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Run the model on this compute resource</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756866126634
        }
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"オーストラリアの首都はどこですか?\"},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
        "response = tokenizer.batch_decode(output_ids)[0]\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5> Save Training script to src folder </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756866757124
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"src\", exist_ok=True)\n",
        "\n",
        "%%writefile ./src/train.py\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Mxfp4Config,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_file\", type=str, required=True,\n",
        "                   help=\"Converted Harmony data (parquet or jsonl)\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./outputs\",\n",
        "                   help=\"Output directory\")\n",
        "    parser.add_argument(\"--push_to_hub\", action=\"store_true\",\n",
        "                   help=\"Specify to push to Hugging Face Hub\")\n",
        "    parser.add_argument(\"--hub_model_id\", type=str, default=None,\n",
        "                   help=\"Model name on Hub (org/name). If omitted, uses output_dir name\")\n",
        "    parser.add_argument(\"--hf_token\", type=str, default=None,\n",
        "                   help=\"Hugging Face access token (or via env var HF_TOKEN)\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=2e-4)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
        "    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=2)\n",
        "    parser.add_argument(\"--grad_accum_steps\", type=int, default=8)\n",
        "    parser.add_argument(\"--max_seq_len\", type=int, default=2048)\n",
        "    parser.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n",
        "    parser.add_argument(\"--cosine_min_lr_rate\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=10)\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def load_harmony_dataset(train_file):\n",
        "    # Identify by file extension\n",
        "    train_file = os.path.abspath(train_file)\n",
        "    if train_file.endswith(\".parquet\"):\n",
        "        ds = load_dataset(\"parquet\", data_files=train_file)[\"train\"]\n",
        "    elif train_file.endswith(\".jsonl\") or train_file.endswith(\".json\"):\n",
        "        ds = load_dataset(\"json\", data_files=train_file, split=\"train\")\n",
        "    else:\n",
        "        raise ValueError(\"train_file must be .parquet or .jsonl\")\n",
        "    if \"messages\" not in ds.column_names:\n",
        "        raise ValueError(\"'messages' column not found. Please convert to Harmony format.\")\n",
        "    return ds\n",
        "\n",
        "\n",
        "def build_formatting_func(tokenizer):\n",
        "    # Convert Harmony messages -> text (trainer will tokenize internally)\n",
        "    def _fmt(example):\n",
        "        msgs = example[\"messages\"]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            msgs,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,  # For supervised fine-tuning include the final response\n",
        "        )\n",
        "        return text\n",
        "    return _fmt\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = get_args()\n",
        "\n",
        "    # Hub token (needed only when pushing)\n",
        "    hf_token = args.hf_token or os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "    # --- Model & Tokenizer ---\n",
        "    # Use MXFP4 in-quantized (dequantize=False recommended) to save memory\n",
        "    quantization_config = Mxfp4Config(dequantize=False)\n",
        "\n",
        "    # Flexible dtype/device so it runs even on a CPU smoke test\n",
        "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    torch_dtype = torch.bfloat16 if use_bf16 else torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    # In AzureML distributed environments accelerate is used, so avoid specifying device_map\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"openai/gpt-oss-20b\",\n",
        "        attn_implementation=\"eager\",\n",
        "        torch_dtype=torch_dtype,\n",
        "        quantization_config=quantization_config,\n",
        "        use_cache=False,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"openai/gpt-oss-20b\",\n",
        "        trust_remote_code=True,\n",
        "        use_fast=True,\n",
        "    )\n",
        "    # Minimal settings for chat / long text\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=\"all-linear\",\n",
        "        target_parameters=[\n",
        "            \"7.mlp.experts.gate_up_proj\",\n",
        "            \"7.mlp.experts.down_proj\",\n",
        "            \"15.mlp.experts.gate_up_proj\",\n",
        "            \"15.mlp.experts.down_proj\",\n",
        "            \"23.mlp.experts.gate_up_proj\",\n",
        "            \"23.mlp.experts.down_proj\",\n",
        "        ],\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # --- Dataset ---\n",
        "    train_ds = load_harmony_dataset(args.train_file)\n",
        "    formatting_func = build_formatting_func(tokenizer)\n",
        "\n",
        "    # --- SFT settings ---\n",
        "    # report_to can be \"none\" / \"tensorboard\" / \"wandb\" etc\n",
        "    training_args = SFTConfig(\n",
        "        learning_rate=args.lr,\n",
        "        gradient_checkpointing=True,\n",
        "        #gradient_checkpointing_kwargs={\"use_reentrant\": False},  # added\n",
        "        #ddp_find_unused_parameters=False,                        # may cause OOM\n",
        "        #packing=False,                                           # added\n",
        "        ##max_seq_length=args.max_seq_len,                         # may hit trl bug\n",
        "        num_train_epochs=args.epochs,\n",
        "        logging_steps=args.logging_steps,\n",
        "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=args.grad_accum_steps,\n",
        "        warmup_ratio=args.warmup_ratio,\n",
        "        lr_scheduler_type=\"cosine_with_min_lr\",\n",
        "        lr_scheduler_kwargs={\"min_lr_rate\": args.cosine_min_lr_rate},\n",
        "        output_dir=args.output_dir,\n",
        "        report_to=\"trackio\", # changed from none\n",
        "        bf16=use_bf16,  # enabled only if GPU supports bf16\n",
        "        fp16=(torch.cuda.is_available() and not use_bf16),\n",
        "        seed=args.seed,\n",
        "        push_to_hub=args.push_to_hub,\n",
        "        hub_model_id=(args.hub_model_id or os.path.basename(args.output_dir)) if args.push_to_hub else None,\n",
        "        hub_token=hf_token if args.push_to_hub else None,\n",
        "        save_total_limit=2,\n",
        "        save_steps=500,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        processing_class=tokenizer,\n",
        "        formatting_func=formatting_func,  # Convert messages to a plain string\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # === Merge LoRA into base model and save ===\n",
        "    try:\n",
        "        print(\"Merging LoRA adapter into base model...\")\n",
        "        merged_model = model.merge_and_unload()\n",
        "        merged_dir = os.path.join(args.output_dir, \"merged\")\n",
        "        os.makedirs(merged_dir, exist_ok=True)\n",
        "        merged_model.save_pretrained(merged_dir)\n",
        "        tokenizer.save_pretrained(merged_dir)\n",
        "        print(f\"Merged model saved at: {merged_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not merge model automatically: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Start Training Job</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756976724768
        }
      },
      "outputs": [],
      "source": [
        "# job configuration\n",
        "NUM_NODES = 1\n",
        "NUM_GPU_PER_NODE = 1\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",  # Root (refers to src/train.py)\n",
        "    command=(\n",
        "        \"python train.py \"\n",
        "        \"--train_file ${{inputs.train_file}} \"\n",
        "        #\"--output_dir ${{outputs.model_dir}} \"   # default is ./outputs\n",
        "        \"--epochs 8 --per_device_train_batch_size 2 --grad_accum_steps 8\"\n",
        "    ),\n",
        "    inputs={\n",
        "        \"train_file\": Input(\n",
        "            type=AssetTypes.URI_FILE,\n",
        "            path=\"apto_reasoning_harmony@latest\"\n",
        "        )\n",
        "    },\n",
        "    outputs={\"model_dir\": {\"mode\": \"rw_mount\", \"path\": \"azureml://datastores/workspaceblobstore/paths/models/gpt-oss-20b-jp-reasoner\"}},\n",
        "    environment=\"env-gpt-oss-01:2\",  # Contains transformers>=4.44, trl>=0.9, peft>=0.10, accelerate>=0.33\n",
        "    compute=COMPUTE_CLUSTER,\n",
        "    display_name=\"gpt-oss-20b-01\",\n",
        "    experiment_name=\"gpt-oss-20b-01-exp\",\n",
        "    instance_count=NUM_NODES,\n",
        "    distribution=PyTorchDistribution(\n",
        "        process_count_per_instance=NUM_GPU_PER_NODE,\n",
        "        node_count=NUM_NODES\n",
        "    ),\n",
        "    environment_variables={\n",
        "        \"NCCL_DEBUG\": \"WARN\",\n",
        "        #\"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
        "        \"HF_TOKEN\": HF_TOKEN,\n",
        "        \"HF_HOME\": \"./outputs/hfhome\",\n",
        "        \"TRACKIO_PROJECT\": \"ft-project\"\n",
        "    }\n",
        ")\n",
        "returned_job = ml_client.jobs.create_or_update(job)\n",
        "print(returned_job.studio_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Register Model</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756961234276
        }
      },
      "outputs": [],
      "source": [
        "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
        "    returned_job.name, \"artifacts/outputs/merged\"\n",
        ")\n",
        "\n",
        "print(\"path to register model: \", model_path_from_job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756961279673
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "model = Model(\n",
        "    path=model_path_from_job,  # Reference to training job output\n",
        "    name=\"gpt-oss-20b-jp-reasoner-01-pre\",\n",
        "    description=\"LoRA fine-tuned gpt-oss-20b model for Japanese reasoning\",\n",
        "    type=\"custom_model\",   # Hugging Face models are commonly registered as custom_model\n",
        "    version=\"3\",             # Explicit version (or omit for auto increment)\n",
        ")\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model)\n",
        "print(f\"Registered: {registered_model.name}:{registered_model.version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Deploy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ./src/score.py\n",
        "# score.py\n",
        "import os, json, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "REASONING_LANGUAGE_DEFAULT = \"Japanese\"\n",
        "\n",
        "def init():\n",
        "    global model, tokenizer\n",
        "    model_root = os.environ.get(\"AZUREML_MODEL_DIR\", \".\")\n",
        "    model_dir  = os.path.join(model_root, os.getenv(\"MODEL_SUBDIR\", \"merged\"))  # Read merged/\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_dir, trust_remote_code=True, local_files_only=True\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=(torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else None),\n",
        "        device_map=(\"auto\" if torch.cuda.is_available() else None),\n",
        "        local_files_only=True,\n",
        "    )\n",
        "    model.eval()\n",
        "\n",
        "def _build_messages(data):\n",
        "    # Input can be either 1) {\"prompt\": \"...\"} or 2) {\"messages\": [...]} \n",
        "    system_prompt = data.get(\n",
        "        \"system_prompt\",\n",
        "        f\"reasoning language: {data.get('reasoning_language', REASONING_LANGUAGE_DEFAULT)}\"\n",
        "    )\n",
        "\n",
        "    if \"messages\" in data and isinstance(data[\"messages\"], list):\n",
        "        msgs = data[\"messages\"]\n",
        "        # Prepend system if the first message isn't system\n",
        "        if not msgs or msgs[0].get(\"role\") != \"system\":\n",
        "            msgs = [{\"role\": \"system\", \"content\": system_prompt}] + msgs\n",
        "        return msgs\n",
        "\n",
        "    # Default messages if only a \"prompt\" was provided\n",
        "    user_prompt = data.get(\"prompt\", \"\")\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "def run(raw_data):\n",
        "    try:\n",
        "        data = json.loads(raw_data) if isinstance(raw_data, str) else raw_data\n",
        "\n",
        "        # ---- Build messages (based on provided code) ----\n",
        "        messages = _build_messages(data)\n",
        "\n",
        "        # Generation parameters (lightweight defaults)\n",
        "        max_new = int(data.get(\"max_new_tokens\", 128))\n",
        "        do_sample = bool(data.get(\"do_sample\", False))\n",
        "        gen_kwargs = {\n",
        "            \"max_new_tokens\": max_new,\n",
        "            \"do_sample\": do_sample,\n",
        "        }\n",
        "        # Only set temperature if sampling; otherwise it triggers a warning\n",
        "        if do_sample and \"temperature\" in data:\n",
        "            gen_kwargs[\"temperature\"] = float(data[\"temperature\"])\n",
        "\n",
        "        # ---- chat template -> input_ids ----\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(model.device)\n",
        "\n",
        "        # ---- Generate ----\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(input_ids, **gen_kwargs)\n",
        "\n",
        "        # ---- Remove prompt echo (keep only the newly generated part) ----\n",
        "        generated_ids = output_ids[0, input_ids.shape[-1]:]\n",
        "        text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Return JSON UTF-8 (simplifies client display)\n",
        "        return json.dumps({\"output\": text}, ensure_ascii=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return error also as JSON\n",
        "        return json.dumps({\"error\": str(e)}, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Create Endpoint</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756961174089
        }
      },
      "outputs": [],
      "source": [
        "endpoint_name = f\"gptoss-jp-{datetime.datetime.now():%m%d%H%M}\"\n",
        "\n",
        "# 1) Create endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=endpoint_name, \n",
        "    description=\"gpt-oss-20b\",\n",
        "    auth_mode=\"key\")\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Deploy the model</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756968716845
        }
      },
      "outputs": [],
      "source": [
        "# 2) Deploy (refer to the already registered \"merged\" model in Model Registry)\n",
        "registered = ml_client.models.get(\"gpt-oss-20b-jp-reasoner-01-pre\", version=\"3\")\n",
        "\n",
        "deploy = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=registered.id,\n",
        "    environment=\"env-gpt-oss-01@latest\",\n",
        "    code_configuration=CodeConfiguration(code=\"./src\", scoring_script=\"score.py\"),\n",
        "    instance_type=\"Standard_NC40ads_H100_v5\", \n",
        "    instance_count=1,\n",
        "    request_settings=OnlineRequestSettings(\n",
        "        request_timeout_ms=180000,\n",
        "        max_concurrent_requests_per_instance=1,\n",
        "        max_queue_wait_ms=180000,\n",
        "    ),\n",
        ")\n",
        "ml_client.begin_create_or_update(deploy).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Change the traffic</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756964014316
        }
      },
      "outputs": [],
      "source": [
        "# 3) Traffic routing\n",
        "ep = ml_client.online_endpoints.get(endpoint_name)\n",
        "ep.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(ep).result()\n",
        "\n",
        "print(\"endpoint:\", endpoint_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Send Request </h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756968996353
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import OnlineRequestSettings\n",
        "from azure.ai.ml import MLClient\n",
        "import json, requests\n",
        "\n",
        "keys = ml_client.online_endpoints.get_keys(name=endpoint_name).primary_key\n",
        "scoring_url = ml_client.online_endpoints.get(endpoint_name).scoring_uri\n",
        "headers = {\"Authorization\": f\"Bearer {keys}\"}\n",
        "\n",
        "payload = {\n",
        "  \"messages\": [\n",
        "    {\"role\": \"system\", \"content\": \"reasoning language: Japanese\"},\n",
        "    {\"role\": \"user\", \"content\": \"オーストラリアの首都はどこですか？\"}\n",
        "  ],\n",
        "  \"max_new_tokens\": 500\n",
        "}\n",
        "\n",
        "res = requests.post(scoring_url, headers=headers, json=payload, timeout=300)\n",
        "print(res.status_code)\n",
        "data = res.json()                 # First-level decode\n",
        "if isinstance(data, str):         # If still a string, decode second-level\n",
        "    data = json.loads(data)\n",
        "\n",
        "print(data[\"output\"])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h5>Delete Endpoint</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756969946808
        }
      },
      "outputs": [],
      "source": [
        "ml_client.online_endpoints.begin_delete(name=endpoint_name).wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>Problem Detection</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756964968400
        }
      },
      "outputs": [],
      "source": [
        "print(res.status_code)\n",
        "print(res.headers.get(\"content-type\"))\n",
        "print(res.text[:1000])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756964883410
        }
      },
      "outputs": [],
      "source": [
        "ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=endpoint_name, lines=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "<h5>End of notebook</h5>"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
